\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\m}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\pp}[2]{\frac{\partial^2 f}{\partial #1 \partial #2}}

%% Title Information %%
\title{Newton-Raphson Method for Convex Optimization}
\author{Conner DiPaolo, Jeffrey Rutledge, Colin Adams}
\institute{Harvey Mudd College}
\date{February, 2016}


\begin{document}

\frame{\titlepage}

%% How To Find Roots %%
\begin{frame}
    \frametitle{Finding Roots of Functions}
    Given a differentiable function $f :
    \RR \mapsto \RR$ we want to find the
    instances when $f(x) = 0$ (not generally
    solvable in closed form).

    \textbf{Newton's Method:}
    \begin{enumerate}
        \item Take a starting position $x_0$
        \item Find where the tangent line $y = f(x_n) + (x_{n+1}-x_n)f^\prime(x_n)$
              is $0$ and iterate:
              \begin{align*}
                y &= f(x_n) + (x_{n+1}-x_n)f^\prime(x_n) = 0\\
                x_{n+1} &= x_n - \frac{f(x_n)}{f^\prime(x_n)}\\
              \end{align*}
    \end{enumerate}

\end{frame}

%% Plots of Root Finding %%
\begin{frame}
    \frametitle{Finding Roots of Functions}
    Initial $x_0 = 0.4$, $f(x) = (x-1) (x-3)^2
    +e^{\left.\frac{1}{3}\right/x}-\cos\left(
    \frac{x}{2}\right)-1.5$\\
    \only<4>{Converges in $28$ iterations.}
    \begin{center}
        \includegraphics<1>[width=3in]{plots/root_plt_0.png}
        \includegraphics<2>[width=3in]{plots/root_plt_1.png}
        \includegraphics<3>[width=3in]{plots/root_plt_2.png}
        \includegraphics<4>[width=3in]{plots/root_plt_3.png}
    \end{center}
    \begin{align*}
        \only<1|only@1>{x_0 = 0.4 \;\;\; \Delta = 0.9805}
        \only<2|only@1>{x_1 = 0.6866 \;\;\; \Delta = 0.6938}
        \only<3|only@1>{x_2 = 0.9938 \;\;\; \Delta = 0.3866}
        \only<4|only@1>{x_3 = 1.2050 \;\;\; \Delta = 0.1754}
    \end{align*}
\end{frame}

%% Using Newton's Method to Optimize Functions %%
\begin{frame}
    \frametitle{Optimizing Function Using Newton's Method}
    Instead of finding where $f(x)=0$, how 
    can we find where $f^\prime(x)=0$?
    \\[30pt]
    \pause
    Instead of:
    \[
        x_{n+1} = x_n - \frac{f(x_n)}{f^\prime(x_n)}
    \]
    \\[10pt]
    Use derivatives:
    \[
        x_{n+1} = x_n - \frac{f^\prime(x_n)}{f^{\prime\prime}(x_n)}
    \]
\end{frame}

%% Using Newton's Method to Optimize %%
%%               f : R^n -> R        %%
\begin{frame}
    \frametitle{Optimizing $f : \RR^n \mapsto \RR$}
    How do we find the derivatives of functions where $f : \RR^n \mapsto \RR?$
    \pause
    \[
        x_{n+1} = x_n - \frac{f^\prime(x_n)}{f^{\prime\prime}(x_n)}
    \]
    becomes,
    \[
        x_{n+1} = x_n - H_f^{-1}(x_n) \nabla f(x_n)
    \]
    \pause
    \textbf{Def:} The gradient of $f: \RR^n \mapsto \RR$ is
    \[
        \nabla f = \m{\frac{\partial}{\partial x_1}f & \frac{\partial}{\partial x_2}f & \dots & \frac{\partial}{\partial x_n}f}
    \]
    \pause
    \textbf{Def:} The Hessian of $f: \RR^n \mapsto \RR$ is
    \[
        H_f = \m{\pp{x_1}{x_1}&\pp{x_1}{x_2}&\dots&\pp{x_1}{x_n}\\
                 \pp{x_2}{x_1}&\ddots&&\pp{x_2}{x_n}\\
        \vdots&&\ddots&\vdots\\
                 \pp{x_n}{x_1}&\pp{x_n}{x_2}&\dots&\pp{x_n}{x_n}}
    \]
\end{frame}

%% Plots of Optimizing f: R^2 -> R   %%
\begin{frame}
    \frametitle{Optimizing $f : \RR^n \mapsto \RR$}
\end{frame}

\end{document}
